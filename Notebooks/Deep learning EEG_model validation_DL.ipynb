{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to validate a deep learning model. At the top the data and model can be loaded into memory, and in the following cells function for validation can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, fnmatch, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from config import PATH_RAW_DATA, PATH_DATA_PROCESSED_DL, PATH_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing data, model, and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AVERAGE = 30\n",
    "MODEL_NAME = 'Fully_connected_regressor_02.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Get all the files in the output folder\n",
    "file_names = os.listdir(PATH_DATA_PROCESSED_DL)\n",
    "\n",
    "# Step 2: Get the full paths of the files (without extensions)\n",
    "files = [os.path.splitext(os.path.join(PATH_DATA_PROCESSED_DL, file_name))[0] for file_name in fnmatch.filter(file_names, \"*.zarr\")]\n",
    "\n",
    "# Step 3: Load all the metadata\n",
    "frames = []\n",
    "\n",
    "for idx, feature_file in enumerate(files):\n",
    "    df_metadata = pd.read_csv(feature_file.replace(\"processed_raw_\", \"processed_metadata_\") + \".csv\")\n",
    "    frames.append(df_metadata)\n",
    "\n",
    "df_metadata = pd.concat(frames) \n",
    "\n",
    "# Step 4: Add missing age information based on the age group the subject is in\n",
    "df_metadata['age_months'].fillna(df_metadata['age_group'], inplace=True)\n",
    "df_metadata['age_days'].fillna(df_metadata['age_group']*30, inplace=True)\n",
    "df_metadata['age_years'].fillna(df_metadata['age_group']/12, inplace=True)\n",
    "\n",
    "# Step 5: List all the unique subject IDs\n",
    "subject_ids = list(set(df_metadata[\"code\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDs_train, IDs_temp = train_test_split(subject_ids, test_size=0.3, random_state=42)\n",
    "IDs_test, IDs_val = train_test_split(IDs_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_generator import DataGenerator\n",
    "\n",
    "train_generator = DataGenerator(list_IDs = IDs_train,\n",
    "                                BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                                metadata = df_metadata,\n",
    "                                n_average = N_AVERAGE,\n",
    "                                batch_size = 10,\n",
    "                                iter_per_epoch = 30,\n",
    "                                n_timepoints = 501, \n",
    "                                n_channels=30, \n",
    "                                shuffle=True)\n",
    "\n",
    "train_generator_noise = DataGenerator(list_IDs = IDs_train,\n",
    "                                      BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                                      metadata = df_metadata,\n",
    "                                      n_average = N_AVERAGE,\n",
    "                                      batch_size = 10,\n",
    "                                      gaussian_noise=0.01,\n",
    "                                      iter_per_epoch = 30,\n",
    "                                      n_timepoints = 501, \n",
    "                                      n_channels=30, \n",
    "                                      shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(list_IDs = IDs_val,\n",
    "                              BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                              metadata = df_metadata,\n",
    "                              n_average = N_AVERAGE,\n",
    "                              batch_size = 10,\n",
    "                              iter_per_epoch = 100,\n",
    "                              n_timepoints = 501,\n",
    "                              n_channels=30,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_generator = DataGenerator(list_IDs = IDs_test,\n",
    "                               BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                               metadata = df_metadata,\n",
    "                               n_average = N_AVERAGE,\n",
    "                               batch_size = 10,\n",
    "                               iter_per_epoch = 100,\n",
    "                               n_timepoints = 501,\n",
    "                               n_channels=30,\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "      <th>age_group</th>\n",
       "      <th>age_days</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>023_35_mc_mmn36</td>\n",
       "      <td>35</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>35.066667</td>\n",
       "      <td>2.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>337_23_jc_mmn_36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>692.0</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>1.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>456_23_md_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>691.0</td>\n",
       "      <td>23.033333</td>\n",
       "      <td>1.919444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>328_23_jc_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>699.0</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>1.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>314_29_mmn_36_wk</td>\n",
       "      <td>29</td>\n",
       "      <td>877.0</td>\n",
       "      <td>29.233333</td>\n",
       "      <td>2.436111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                           cnt_path  \\\n",
       "0    23  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   337  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   456  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   328  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   314  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "\n",
       "              cnt_file  age_group  age_days  age_months  age_years  \n",
       "0      023_35_mc_mmn36         35    1052.0   35.066667   2.922222  \n",
       "0  337_23_jc_mmn_36_wk         23     692.0   23.066667   1.922222  \n",
       "0   456_23_md_mmn36_wk         23     691.0   23.033333   1.919444  \n",
       "0   328_23_jc_mmn36_wk         23     699.0   23.300000   1.941667  \n",
       "0     314_29_mmn_36_wk         29     877.0   29.233333   2.436111  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_path = os.path.join(PATH_MODELS, MODEL_NAME)\n",
    "loaded_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    \"\"\" Evaluates the model \"\"\"\n",
    "    model.evaluate(train_generator)\n",
    "    model.evaluate(val_generator)\n",
    "    model.evaluate(test_generator)\n",
    "    \n",
    "def print_few_predictions(model):\n",
    "    \"\"\" Prints a few predictions, as a sanity check \"\"\"\n",
    "    x_test, y_test = test_generator.__getitem__(0)\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    for idx in range(len(y_test)): print(f\"{y_test[idx]} -> {predictions[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of error stability (Vandenbosch et al., 2018): \n",
    "\n",
    "_\"Stability was assessed as the correlation between the prediction errors (estimated minus actual age) of subjects at baseline with their own prediction error at follow-up.\"_\n",
    "\n",
    "\n",
    "I think this means: Take the prediction error (of a subject) at time 1 and compare it to the prediction error at time 2, take the prediction error at time 2 and compare it to time 3, take the prediction error at time 3 and compare it to time 4. You can then make two lists of errors: TIME_N and TIME_N+1 and look at the correlation between those two. \n",
    "\n",
    "A stable error then would mean a positive correlation, because if error is low at time n, you expect it to be low at time n+1 as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for ID: 712..\n",
      "Predicting for ID: 420..\n",
      "Predicting for ID: 758..\n",
      "Predicting for ID: 28..\n",
      "Predicting for ID: 732..\n",
      "Predicting for ID: 613..\n",
      "Predicting for ID: 164..\n",
      "Predicting for ID: 709..\n",
      "Predicting for ID: 121..\n",
      "Predicting for ID: 711..\n",
      "Predicting for ID: 329..\n",
      "Predicting for ID: 169..\n",
      "Predicting for ID: 474..\n",
      "Predicting for ID: 154..\n",
      "Predicting for ID: 428..\n",
      "Predicting for ID: 159..\n",
      "Predicting for ID: 472..\n",
      "Predicting for ID: 632..\n",
      "Predicting for ID: 451..\n",
      "Predicting for ID: 426..\n",
      "Predicting for ID: 158..\n",
      "Predicting for ID: 122..\n",
      "Predicting for ID: 496..\n",
      "Predicting for ID: 485..\n",
      "Predicting for ID: 425..\n",
      "Predicting for ID: 149..\n",
      "Predicting for ID: 317..\n",
      "Predicting for ID: 105..\n",
      "Predicting for ID: 301..\n",
      "Predicting for ID: 304..\n",
      "Predicting for ID: 310..\n",
      "Predicting for ID: 135..\n",
      "Predicting for ID: 641..\n",
      "Predicting for ID: 719..\n",
      "Predicting for ID: 108..\n",
      "Predicting for ID: 466..\n",
      "Predicting for ID: 156..\n",
      "Predicting for ID: 29..\n",
      "Predicting for ID: 733..\n",
      "Predicting for ID: 455..\n",
      "Predicting for ID: 129..\n",
      "Predicting for ID: 343..\n",
      "Predicting for ID: 751..\n",
      "Predicting for ID: 25..\n",
      "Predicting for ID: 163..\n",
      "Predicting for ID: 433..\n",
      "Pearsons correlation: 0.795\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def error_stability(model, IDs_test):\n",
    "    \"\"\"Takes in the IDs of the test subjects, calculates the error stability per subject\n",
    "    and returns this as a dictionary\"\"\"\n",
    "    \n",
    "    errors_time_N = []\n",
    "    errors_time_N1 = []\n",
    "    \n",
    "    # Step 1: Iterate over subjects\n",
    "    for ID in IDs_test:\n",
    "        print(f\"Predicting for ID: {ID}..\")\n",
    "        \n",
    "        # Step 2: Find all files of a subject\n",
    "        df_temp = df_metadata[df_metadata['code'] == ID]\n",
    "    \n",
    "        # Step 3: Find all the age groups the subject was found in\n",
    "        ages_subject = sorted(list(set(df_temp['age_group'].tolist())))\n",
    "        \n",
    "        if len(ages_subject) == 1:\n",
    "            continue\n",
    "                    \n",
    "        # Step 4: Loop over all the ages of a subject\n",
    "        prev_prediction_error = None\n",
    "        curr_prediction_error = None\n",
    "                       \n",
    "        for age_group in ages_subject:\n",
    "            prev_prediction_error = curr_prediction_error\n",
    "            \n",
    "            X_data = np.zeros((0, 30, 501))\n",
    "            \n",
    "            # Step 5: Concatenate data of files in the same age group before averaging all epochs\n",
    "            for i, metadata_file in df_temp[df_temp['age_group'] == age_group].iterrows():\n",
    "                filename = os.path.join(PATH_DATA_PROCESSED_DL, 'processed_raw_' + metadata_file['cnt_file'] + '.zarr')\n",
    "                data_signal = zarr.open(os.path.join(filename), mode='r')\n",
    "                                \n",
    "                X_data = np.concatenate((X_data, data_signal), axis=0) \n",
    "            \n",
    "            X_data_mean = np.mean(X_data[:,:,:], axis=0) # Average all epochs of a subject at a specific age\n",
    "            X = np.expand_dims(X_data_mean, axis=0)            \n",
    "            X = np.swapaxes(X, 1, 2)\n",
    "                                    \n",
    "            actual_age = df_temp[df_temp['age_group'] == age_group]['age_months'].values[0]\n",
    "            curr_prediction_error = model.predict(X).flatten()[0] - actual_age\n",
    "#             curr_prediction_error = model.predict(X)\n",
    "#             print(curr_prediction_error)\n",
    "            \n",
    "#             curr_prediction_error = curr_prediction_error.flatten()[0] - actual_age\n",
    "            \n",
    "                        \n",
    "            # Step 6: If there are two values to compare, i.e. time N and time N+1, add them to the lists\n",
    "            if prev_prediction_error and curr_prediction_error:\n",
    "                \n",
    "                errors_time_N.append(prev_prediction_error)\n",
    "                errors_time_N1.append(curr_prediction_error)\n",
    "                \n",
    "    # Step 7: Look at correlation between prediction error and follow-up\n",
    "    corr, _ = pearsonr(errors_time_N, errors_time_N1)\n",
    "    print(f\"Pearsons correlation: {corr:.3f}\")        \n",
    "    \n",
    "error_stability(loaded_model, IDs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for ID: 712..\n",
      "Predicting for ID: 420..\n",
      "Predicting for ID: 758..\n",
      "Predicting for ID: 28..\n",
      "Predicting for ID: 732..\n",
      "Predicting for ID: 613..\n",
      "Predicting for ID: 164..\n",
      "Predicting for ID: 709..\n",
      "Predicting for ID: 121..\n",
      "Predicting for ID: 711..\n",
      "Predicting for ID: 329..\n",
      "Predicting for ID: 169..\n",
      "Predicting for ID: 474..\n",
      "Predicting for ID: 154..\n",
      "Predicting for ID: 428..\n",
      "Predicting for ID: 159..\n",
      "Predicting for ID: 472..\n",
      "Predicting for ID: 632..\n",
      "Predicting for ID: 451..\n",
      "Predicting for ID: 426..\n",
      "Predicting for ID: 158..\n",
      "Predicting for ID: 122..\n",
      "Predicting for ID: 496..\n",
      "Predicting for ID: 485..\n",
      "Predicting for ID: 425..\n",
      "Predicting for ID: 149..\n",
      "Predicting for ID: 317..\n",
      "Predicting for ID: 105..\n",
      "Predicting for ID: 301..\n",
      "Predicting for ID: 304..\n",
      "Predicting for ID: 310..\n",
      "Predicting for ID: 135..\n",
      "Predicting for ID: 641..\n",
      "Predicting for ID: 719..\n",
      "Predicting for ID: 108..\n",
      "Predicting for ID: 466..\n",
      "Predicting for ID: 156..\n",
      "Predicting for ID: 29..\n",
      "Predicting for ID: 733..\n",
      "Predicting for ID: 455..\n",
      "Predicting for ID: 129..\n",
      "Predicting for ID: 343..\n",
      "Predicting for ID: 751..\n",
      "Predicting for ID: 25..\n",
      "Predicting for ID: 163..\n",
      "Predicting for ID: 433..\n",
      "Pearsons correlation: 0.674\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def error_stability(model, IDs_test):\n",
    "    \"\"\"Takes in the IDs of the test subjects, calculates the error stability per subject\n",
    "    and returns this as a dictionary\"\"\"\n",
    "    \n",
    "    errors_time_N = []\n",
    "    errors_time_N1 = []\n",
    "    \n",
    "    # Step 1: Iterate over subjects\n",
    "    for ID in IDs_test:\n",
    "        print(f\"Predicting for ID: {ID}..\")\n",
    "        \n",
    "        # Step 2: Find all files of a subject\n",
    "        df_temp = df_metadata[df_metadata['code'] == ID]\n",
    "    \n",
    "        # Step 3: Find all the age groups the subject was found in\n",
    "        ages_subject = sorted(list(set(df_temp['age_group'].tolist())))\n",
    "        \n",
    "        if len(ages_subject) == 1:\n",
    "            continue\n",
    "                    \n",
    "        # Step 4: Loop over all the ages of a subject\n",
    "        prev_prediction_error = None\n",
    "        curr_prediction_error = None\n",
    "                       \n",
    "        for age_group in ages_subject:\n",
    "            prev_prediction_error = curr_prediction_error\n",
    "            \n",
    "            X_data = np.zeros((0, 30, 501))\n",
    "            \n",
    "            # Step 5: Concatenate data of files in the same age group before averaging all epochs\n",
    "            for i, metadata_file in df_temp[df_temp['age_group'] == age_group].iterrows():\n",
    "                filename = os.path.join(PATH_DATA_PROCESSED_DL, 'processed_raw_' + metadata_file['cnt_file'] + '.zarr')\n",
    "                data_signal = zarr.open(os.path.join(filename), mode='r')\n",
    "                                \n",
    "                X_data = np.concatenate((X_data, data_signal), axis=0) \n",
    "            \n",
    "            X_data_mean = np.mean(X_data[:,:,:], axis=0) # Average all epochs of a subject at a specific age\n",
    "            X = np.expand_dims(X_data_mean, axis=0)            \n",
    "            X = np.swapaxes(X, 1, 2)\n",
    "                                    \n",
    "            actual_age = df_temp[df_temp['age_group'] == age_group]['age_months'].values[0]\n",
    "            curr_prediction_error = model.predict(X).flatten()[0] - actual_age\n",
    "                        \n",
    "            # Step 6: If there are two values to compare, i.e. time N and time N+1, add them to the lists\n",
    "            if prev_prediction_error and curr_prediction_error:\n",
    "                \n",
    "                errors_time_N.append(prev_prediction_error)\n",
    "                errors_time_N1.append(curr_prediction_error)\n",
    "                \n",
    "    # Step 7: Look at correlation between prediction error and follow-up\n",
    "    corr, _ = pearsonr(errors_time_N, errors_time_N1)\n",
    "    print(f\"Pearsons correlation: {corr:.3f}\")        \n",
    "    \n",
    "error_stability(loaded_model, IDs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 66s 462ms/step - loss: 34.7103 - root_mean_squared_error: 5.8915 - mean_absolute_error: 4.5385\n",
      "30/30 [==============================] - 13s 437ms/step - loss: 64.1352 - root_mean_squared_error: 8.0084 - mean_absolute_error: 6.1294\n",
      "35/35 [==============================] - 14s 398ms/step - loss: 72.1462 - root_mean_squared_error: 8.4939 - mean_absolute_error: 6.3563\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.9] -> [27.630173]\n",
      "[17.03333333] -> [23.78022]\n",
      "[10.96666667] -> [18.644392]\n",
      "[11.33333333] -> [10.05482]\n",
      "[47.4] -> [30.856066]\n",
      "[40.9] -> [29.112087]\n",
      "[35.5] -> [15.527671]\n",
      "[23.06666667] -> [15.542363]\n",
      "[34.96666667] -> [28.063675]\n",
      "[35.03333333] -> [24.761223]\n"
     ]
    }
   ],
   "source": [
    "print_few_predictions(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_stability(loaded_model, IDs_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
