{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional machine learning models for age prediction on EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses traditional ML methods to predict the age of infants using EEG data. The EEG data is preprocessed and features are extracted as shown in the notebook 'Deep learning EEG_dataset preprocessing'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, fnmatch, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from config import ROOT, PATH_CODE, PATH_DATA, PATH_DATA_PROCESSED, PATH_MODELS, PATH_METADATA, PATH_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Get all the files in the output folder\n",
    "2. Get the full paths of the files without the .h5 or .csv extensions\n",
    "3. Load the features from the .h5 files\n",
    "4. Assign the proper labels to the files based on the metadata\n",
    "5. Assign the subject's code to the files based on the metadata\n",
    "6. Split the data into a training, validation and test set (NOTE: make sure data points from same subjects don't end up in same set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Get all the files in the output folder\n",
    "file_names = os.listdir(PATH_OUTPUT)\n",
    "\n",
    "# Step 2: Get the full paths of the files (without extensions)\n",
    "files = [os.path.splitext(os.path.join(PATH_OUTPUT, file_name))[0] for file_name in fnmatch.filter(file_names, \"*.h5\")]\n",
    "\n",
    "# Step 3: Load the features\n",
    "frames = []\n",
    "\n",
    "for idx, feature_file in enumerate(files):\n",
    "    df_features = pd.read_hdf(feature_file + \".h5\")\n",
    "    df_metadata = pd.read_csv(feature_file.replace(\"extracted_features_\", \"processed_data_\") + \".csv\")\n",
    "    \n",
    "    # Step 4: Assign labels\n",
    "    df_features['label'] = df_metadata['age_months'][0]\n",
    "    \n",
    "    # Step 5: Assign subject code\n",
    "    df_features['code'] = df_metadata['code'][0]\n",
    "    frames.append(df_features)\n",
    "\n",
    "df = pd.concat(frames) \n",
    "\n",
    "# Step 6: Split data in train, validation and test\n",
    "# df_train, df_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "# df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# TODO: Split train/validation/test based on 'code' (subject number) to make sure the same code isn't in multiple sets at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test proportions: 0.849999378655425/0.15000062134457504\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Train/val/test proportions: {len(df_train)/len(df)}/{len(df_val)/len(df)}/{len(df_test)/len(df)}\")\n",
    "print(f\"Train/test proportions: {len(df_train)/len(df)}/{len(df_test)/len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['label', 'code'], axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "# X_val = df_val.drop(['label', 'code'], axis=1)\n",
    "# y_val = df_val['label']\n",
    "\n",
    "X_test = df_test.drop(['label', 'code'], axis=1)\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(df, df_temp, frames, df_features, df_metadata, df_train, df_val, df_test)\n",
    "del(file_names, files, df, frames, df_features, df_metadata, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1953824\n",
      "0.0186048\n",
      "0.740365208\n",
      "0.003283216\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X_train.memory_usage(deep=True).sum()/1000000000}\")\n",
    "print(f\"{y_train.memory_usage(deep=True)/1000000000}\")\n",
    "print(f\"{X_test.memory_usage(deep=True).sum()/1000000000}\")\n",
    "print(f\"{y_test.memory_usage(deep=True)/1000000000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed: 13.6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators': [10, 25, 50, 100], \n",
    "              'max_depth': [5, 10],\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'criterion' :['mse', 'mae'],\n",
    "              'ccp_alpha': list(np.linspace(0, 1, 10))\n",
    "             }\n",
    "\n",
    "rf_clf = GridSearchCV(RandomForestRegressor(verbose=10), parameters, verbose=10, n_jobs=1)\n",
    "# rf_clf.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "score = rf_clf.score(X_test, y_test)\n",
    "\n",
    "# MSE\n",
    "predictions = rf_clf.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(f\"Performance of Random Forest regressor: R-squared = {score} and MSE = {rmse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "parameters = {'C': [0.1, 1, 100, 1000],\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "              'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "              'gamma': ['scale', 'auto', 0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n",
    "}\n",
    "\n",
    "\n",
    "svr_clf = GridSearchCV(SVR(verbose=True), parameters, verbose=10)\n",
    "svr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "score = svr_clf.score(X_test, y_test)\n",
    "\n",
    "# MSE\n",
    "predictions = svr_clf.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(f\"Performance of Support Vector regressor: R-squared = {score} and MSE = {rmse}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
