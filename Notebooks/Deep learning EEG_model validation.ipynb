{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to validate a deep learning model. At the top the data and model can be loaded into memory, and in the following cells function for validation can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, fnmatch, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from config import PATH_RAW_DATA, PATH_DATA_PROCESSED_DL, PATH_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing data, model, and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AVERAGE = 40\n",
    "MODEL_NAME = 'Fully_connected_regressor_01.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Get all the files in the output folder\n",
    "file_names = os.listdir(PATH_DATA_PROCESSED_DL)\n",
    "\n",
    "# Step 2: Get the full paths of the files (without extensions)\n",
    "files = [os.path.splitext(os.path.join(PATH_DATA_PROCESSED_DL, file_name))[0] for file_name in fnmatch.filter(file_names, \"*.zarr\")]\n",
    "\n",
    "# Step 3: Load all the metadata\n",
    "frames = []\n",
    "\n",
    "for idx, feature_file in enumerate(files):\n",
    "    df_metadata = pd.read_csv(feature_file.replace(\"processed_raw_\", \"processed_metadata_\") + \".csv\")\n",
    "    frames.append(df_metadata)\n",
    "\n",
    "df_metadata = pd.concat(frames) \n",
    "\n",
    "# Step 4: Add missing age information based on the age group the subject is in\n",
    "df_metadata['age_months'].fillna(df_metadata['age_group'], inplace=True)\n",
    "df_metadata['age_days'].fillna(df_metadata['age_group']*30, inplace=True)\n",
    "df_metadata['age_years'].fillna(df_metadata['age_group']/12, inplace=True)\n",
    "\n",
    "# Step 5: List all the unique subject IDs\n",
    "subject_ids = list(set(df_metadata[\"code\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDs_train, IDs_temp = train_test_split(subject_ids, test_size=0.3, random_state=42)\n",
    "IDs_test, IDs_val = train_test_split(IDs_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_generator import DataGenerator\n",
    "\n",
    "train_generator = DataGenerator(list_IDs = IDs_train,\n",
    "                                BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                                metadata = df_metadata,\n",
    "                                n_average = N_AVERAGE,\n",
    "                                batch_size = 10,\n",
    "                                iter_per_epoch = 30,\n",
    "                                n_timepoints = 501, \n",
    "                                n_channels=30, \n",
    "                                shuffle=True)\n",
    "\n",
    "train_generator_noise = DataGenerator(list_IDs = IDs_train,\n",
    "                                      BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                                      metadata = df_metadata,\n",
    "                                      n_average = N_AVERAGE,\n",
    "                                      batch_size = 10,\n",
    "                                      gaussian_noise=0.01,\n",
    "                                      iter_per_epoch = 30,\n",
    "                                      n_timepoints = 501, \n",
    "                                      n_channels=30, \n",
    "                                      shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(list_IDs = IDs_val,\n",
    "                              BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                              metadata = df_metadata,\n",
    "                              n_average = N_AVERAGE,\n",
    "                              batch_size = 10,\n",
    "                              iter_per_epoch = 100,\n",
    "                              n_timepoints = 501,\n",
    "                              n_channels=30,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_generator = DataGenerator(list_IDs = IDs_test,\n",
    "                               BASE_PATH = PATH_DATA_PROCESSED_DL,\n",
    "                               metadata = df_metadata,\n",
    "                               n_average = N_AVERAGE,\n",
    "                               batch_size = 10,\n",
    "                               iter_per_epoch = 100,\n",
    "                               n_timepoints = 501,\n",
    "                               n_channels=30,\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>cnt_path</th>\n",
       "      <th>cnt_file</th>\n",
       "      <th>age_group</th>\n",
       "      <th>age_days</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>023_35_mc_mmn36</td>\n",
       "      <td>35</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>35.066667</td>\n",
       "      <td>2.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>337_23_jc_mmn_36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>692.0</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>1.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>456_23_md_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>691.0</td>\n",
       "      <td>23.033333</td>\n",
       "      <td>1.919444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>328_23_jc_mmn36_wk</td>\n",
       "      <td>23</td>\n",
       "      <td>699.0</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>1.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314</td>\n",
       "      <td>/Volumes/Seagate Expansion Drive/ePodium/Data/...</td>\n",
       "      <td>314_29_mmn_36_wk</td>\n",
       "      <td>29</td>\n",
       "      <td>877.0</td>\n",
       "      <td>29.233333</td>\n",
       "      <td>2.436111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                           cnt_path  \\\n",
       "0    23  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   337  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   456  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   328  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "0   314  /Volumes/Seagate Expansion Drive/ePodium/Data/...   \n",
       "\n",
       "              cnt_file  age_group  age_days  age_months  age_years  \n",
       "0      023_35_mc_mmn36         35    1052.0   35.066667   2.922222  \n",
       "0  337_23_jc_mmn_36_wk         23     692.0   23.066667   1.922222  \n",
       "0   456_23_md_mmn36_wk         23     691.0   23.033333   1.919444  \n",
       "0   328_23_jc_mmn36_wk         23     699.0   23.300000   1.941667  \n",
       "0     314_29_mmn_36_wk         29     877.0   29.233333   2.436111  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_path = os.path.join(PATH_MODELS, MODEL_NAME)\n",
    "loaded_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'iter:0' shape=() dtype=int64, numpy=529386>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.optimizer.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    \"\"\" Evaluates the model \"\"\"\n",
    "    model.evaluate(train_generator)\n",
    "    model.evaluate(val_generator)\n",
    "    model.evaluate(test_generator)\n",
    "    \n",
    "def print_few_predictions(model):\n",
    "    \"\"\" Prints a few predictions, as a sanity check \"\"\"\n",
    "    x_test, y_test = test_generator.__getitem__(0)\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    for idx in range(len(y_test)): print(f\"{y_test[idx]} -> {predictions[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of error stability (Vandenbosch et al., 2018): \n",
    "\n",
    "_\"Stability was assessed as the correlation between the prediction errors (estimated minus actual age) of subjects at baseline with their own prediction error at follow-up.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 712 Pearsons correlation: -0.223\n",
      "Subject 420 Pearsons correlation: -0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbruns/anaconda3/envs/mne/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 758 Pearsons correlation: nan\n",
      "Subject 28 Pearsons correlation: -0.689\n",
      "Subject 732 Pearsons correlation: -0.578\n",
      "Subject 613 Pearsons correlation: -0.435\n",
      "Subject 164 Pearsons correlation: -0.556\n",
      "Subject 709 Pearsons correlation: -0.724\n",
      "Subject 121 Pearsons correlation: -0.453\n",
      "Subject 711 Pearsons correlation: -0.816\n",
      "Subject 329 Pearsons correlation: -0.493\n",
      "Subject 169 Pearsons correlation: -0.657\n",
      "Subject 474 Pearsons correlation: -0.812\n",
      "Subject 154 Pearsons correlation: -0.430\n",
      "Subject 428 Pearsons correlation: -0.704\n",
      "Subject 159 Pearsons correlation: -0.546\n",
      "Subject 472 Pearsons correlation: -0.706\n",
      "Subject 632 Pearsons correlation: -0.856\n",
      "Subject 451 Pearsons correlation: -0.473\n",
      "Subject 426 Pearsons correlation: -0.767\n",
      "Subject 158 Pearsons correlation: -0.300\n",
      "Subject 122 Pearsons correlation: -0.911\n",
      "Subject 496 Pearsons correlation: -0.483\n",
      "Subject 485 Pearsons correlation: -0.635\n",
      "Subject 425 Pearsons correlation: -0.701\n",
      "Subject 149 Pearsons correlation: -0.684\n",
      "Subject 317 Pearsons correlation: -0.483\n",
      "Subject 105 Pearsons correlation: -0.511\n",
      "Subject 301 Pearsons correlation: -0.614\n",
      "Subject 304 Pearsons correlation: -0.498\n",
      "Subject 310 Pearsons correlation: -0.749\n",
      "Subject 135 Pearsons correlation: -0.547\n",
      "Subject 641 Pearsons correlation: -0.931\n",
      "Subject 719 Pearsons correlation: -0.010\n",
      "Subject 108 Pearsons correlation: -0.791\n",
      "Subject 466 Pearsons correlation: -0.683\n",
      "Subject 156 Pearsons correlation: -0.694\n",
      "Subject 29 Pearsons correlation: -0.659\n",
      "Subject 733 Pearsons correlation: -0.822\n",
      "Subject 455 Pearsons correlation: -0.462\n",
      "Subject 129 Pearsons correlation: -0.490\n",
      "Subject 343 Pearsons correlation: -0.668\n",
      "Subject 751 Pearsons correlation: -0.134\n",
      "Subject 25 Pearsons correlation: -0.094\n",
      "Subject 163 Pearsons correlation: -0.707\n",
      "Subject 433 Pearsons correlation: -0.893\n",
      "Pearsons correlation combined: -0.588\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def create_averaged_epoch(data_signal):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create averages of N_AVERAGE epochs.\n",
    "    Will create one averaged epoch per found unique label from N_AVERAGE random epochs.\n",
    "\n",
    "    Args:\n",
    "    --------\n",
    "    data_signal: numpy array\n",
    "        Data from one person as numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    X_data = np.zeros((0, 30, 501))\n",
    "    num_epochs = len(data_signal)\n",
    "\n",
    "    if num_epochs >= N_AVERAGE:\n",
    "        select = np.random.choice(num_epochs, N_AVERAGE, replace=False)\n",
    "        signal_averaged = np.mean(data_signal.oindex[select,:,:], axis=0)\n",
    "    else:\n",
    "        signal_averaged = np.mean(data_signal.oindex[:,:,:], axis=0)\n",
    "\n",
    "    X_data = np.concatenate([X_data, np.expand_dims(signal_averaged, axis=0)], axis=0)\n",
    "    return X_data\n",
    "\n",
    "def error_stability(model, IDs_test, N_per_file):\n",
    "    \"\"\"Takes in the IDs of the test subjects, calculates the error stability per subject\n",
    "    and returns this as a dictionary\"\"\"\n",
    "    \n",
    "    combined_errors = []\n",
    "    cobined_ages = []\n",
    "    \n",
    "    # Step 1: Iterate over subjects\n",
    "    for ID in IDs_test:\n",
    "        \n",
    "        # Step 2: Find all files of a subject\n",
    "        df_temp = df_metadata[df_metadata['code'] == ID]\n",
    "        \n",
    "        subject_errors = []\n",
    "        subject_ages = []\n",
    "        \n",
    "        for i, metadata_file in df_temp.iterrows():\n",
    "            \n",
    "            X_data = np.zeros((0, 30, 501))\n",
    "            y_data = []\n",
    "            \n",
    "            for n in range(N_per_file):\n",
    "                filename = os.path.join(PATH_DATA_PROCESSED_DL, 'processed_raw_' + metadata_file['cnt_file'] + '.zarr')\n",
    "                data_signal = zarr.open(os.path.join(filename), mode='r')\n",
    "\n",
    "                X = create_averaged_epoch(data_signal)\n",
    "\n",
    "                X_data = np.concatenate((X_data, X), axis=0)\n",
    "                y_data.append(metadata_file['age_months'])\n",
    "            \n",
    "            # Step 3: Calculate age prediction for N (averaged) epochs of each file\n",
    "            X_data, y_data = np.swapaxes(X_data,1,2), np.array(y_data).reshape((-1,1))\n",
    "            predictions = model.predict(X_data)\n",
    "            errors = list(np.array(predictions.flatten()) - np.array(y_data.flatten()))\n",
    "            \n",
    "            subject_errors.extend(errors)\n",
    "            subject_ages.extend(y_data.flatten())\n",
    "\n",
    "        combined_errors.extend(subject_errors)\n",
    "        cobined_ages.extend(subject_ages)\n",
    "    \n",
    "        # Step 4: Look at predictions for the same age (different files), are they stable\n",
    "        corr, _ = pearsonr(subject_ages, subject_errors)\n",
    "        print(f\"Subject {ID} Pearsons correlation: {corr:.3f}\")\n",
    "        \n",
    "    # Step 5: Look at predictions within-subject over multiple ages, is the error stable?\n",
    "    corr, _ = pearsonr(cobined_ages, combined_errors)\n",
    "    print(f\"Pearsons correlation combined: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac53a1694df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_few_predictions(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_stability(loaded_model, IDs_test, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
